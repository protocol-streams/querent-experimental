{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyONo4A7QT6z4sY0nrTUm7JB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/protocol-streams/querent-experimental/blob/main/Full_end_to_End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "f08sRO3Zpd4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to Google Colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/corpus-data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKHZeSRJn20h",
        "outputId": "13f35cd3-7475-49c6-8eea-68bb0f3a749b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/corpus-data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the PDF Content\n",
        "\n",
        "\n",
        "\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    pdf_reader = PdfReader(pdf_path)\n",
        "    pdf_text = \"\"\n",
        "\n",
        "    for page_num in range(len(pdf_reader.pages)):\n",
        "        page = pdf_reader.pages[page_num]\n",
        "        pdf_text += page.extract_text()\n",
        "\n",
        "    return pdf_text\n",
        "\n",
        "# Replace 'Your_PDF_Name.pdf' with the name of the uploaded PDF\n",
        "pdf_text = extract_text_from_pdf('eScholarship UC item 4m56569q.pdf')\n",
        "print(pdf_text[:1000])  # Print the first 1000 characters to check\n",
        "\n"
      ],
      "metadata": {
        "id": "xDYzkwGXofGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BxEAUx1FHuz",
        "outputId": "650eaa5d-c248-4884-e5df-1d0a47a3c7e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('Gang', 'I-LOC'), ('##ot', 'I-LOC'), ('##ri', 'I-LOC'), ('Per', 'I-ORG'), ('##mal', 'I-ORG'), ('##ink', 'I-ORG'), ('##hip', 'I-ORG'), ('Journal', 'I-ORG'), ('International', 'I-ORG'), ('Journal', 'I-ORG'), ('of', 'I-ORG'), ('Re', 'I-ORG'), ('##mote', 'I-ORG'), ('Sen', 'I-MISC'), ('##sing', 'I-MISC'), ('Sara', 'I-PER'), ('##s', 'I-PER'), ('##wat', 'I-PER'), ('Pune', 'I-PER'), ('##et', 'I-PER'), ('Syed', 'I-PER'), ('Ta', 'I-PER'), ('##j', 'I-PER'), ('##dar', 'I-PER'), ('##ul', 'I-PER'), ('H', 'I-PER'), ('F', 'I-PER'), ('##ami', 'I-PER'), ('##gli', 'I-PER'), ('##etti', 'I-PER'), ('James', 'I-PER'), ('S', 'I-PER'), ('##S', 'I-ORG'), ('California', 'I-ORG'), ('Digital', 'I-ORG'), ('Library', 'I-ORG'), ('University', 'I-ORG'), ('of', 'I-ORG'), ('California', 'I-ORG'), ('##fo', 'I-ORG'), ('##n', 'I-ORG'), ('##line', 'I-ORG'), ('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('Libraries', 'I-ORG'), ('International', 'I-ORG')]\n"
          ]
        }
      ],
      "source": [
        "#Extract Entities using BERT\n",
        "\n",
        "from transformers import BertTokenizer, BertForTokenClassification\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "model = BertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
        "\n",
        "def extract_entities(text):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    entities = []\n",
        "\n",
        "    # Process text in chunks of 510 tokens to account for special tokens\n",
        "    for i in range(0, len(tokens), 510):\n",
        "        chunk = tokens[i:i+510]\n",
        "        inputs = tokenizer.encode(chunk, return_tensors=\"pt\")\n",
        "        outputs = model(inputs).logits\n",
        "        predictions = torch.argmax(outputs, dim=2)[0]\n",
        "\n",
        "        for token, label_id in zip(chunk, predictions[1:-1]):  # Ignore special tokens\n",
        "            label = model.config.id2label[label_id.item()]\n",
        "            if label != \"O\":\n",
        "                entities.append((token, label))\n",
        "    return entities\n",
        "\n",
        "entities = extract_entities(pdf_text)\n",
        "print(entities[:50])  # Print the first 50 entities to check\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_subwords(entities):\n",
        "    merged_entities = []\n",
        "    current_entity = \"\"\n",
        "    current_label = \"\"\n",
        "    for token, label in entities:\n",
        "        if token.startswith(\"##\"):\n",
        "            current_entity += token[2:]\n",
        "        else:\n",
        "            if current_entity:\n",
        "                merged_entities.append((current_entity, current_label))\n",
        "            current_entity = token\n",
        "            current_label = label\n",
        "    if current_entity:\n",
        "        merged_entities.append((current_entity, current_label))\n",
        "    return merged_entities\n",
        "\n",
        "merged_entities = merge_subwords(entities)\n",
        "print(merged_entities[:50])  # Print the first 50 merged entities to check\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcegrtHMy4mJ",
        "outputId": "237f9bdd-d7ec-4d60-9365-a74e2f6a5bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('Gangotri', 'I-LOC'), ('Permalinkhip', 'I-ORG'), ('Journal', 'I-ORG'), ('International', 'I-ORG'), ('Journal', 'I-ORG'), ('of', 'I-ORG'), ('Remote', 'I-ORG'), ('Sensing', 'I-MISC'), ('Saraswat', 'I-PER'), ('Puneet', 'I-PER'), ('Syed', 'I-PER'), ('Tajdarul', 'I-PER'), ('H', 'I-PER'), ('Famiglietti', 'I-PER'), ('James', 'I-PER'), ('SS', 'I-PER'), ('California', 'I-ORG'), ('Digital', 'I-ORG'), ('Library', 'I-ORG'), ('University', 'I-ORG'), ('of', 'I-ORG'), ('Californiafonline', 'I-ORG'), ('UC', 'I-ORG'), ('Irvine', 'I-ORG'), ('Libraries', 'I-ORG'), ('International', 'I-ORG'), ('Journal', 'I-ORG'), ('of', 'I-ORG'), ('Remote', 'I-ORG'), ('Sensingfonline', 'I-MISC'), ('Gangotri', 'I-LOC'), ('Puneet', 'I-PER'), ('Saraswat', 'I-PER'), ('Tajdarul', 'I-PER'), ('H', 'I-PER'), ('.', 'I-PER'), ('Syed', 'I-PER'), ('James', 'I-PER'), ('S', 'I-PER'), ('.', 'I-PER'), ('Famiglietti', 'I-PER'), ('Eric', 'I-PER'), ('J', 'I-PER'), ('.', 'I-PER'), ('Fielding', 'I-PER'), ('Robert', 'I-PER')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get all entities in an excel file\n",
        "\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Convert the entities list to a pandas DataFrame\n",
        "df = pd.DataFrame(merged_entities, columns=['Token', 'Label'])\n",
        "\n",
        "# Write the DataFrame to an Excel file\n",
        "df.to_excel('entities.xlsx', index=False)\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download('entities.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3QJAV7ecyPJ2",
        "outputId": "00993331-df40-4b5d-d73b-7490e67a4c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_893b1c3d-0dbd-48ef-b842-7ed0d1d1a7d5\", \"entities.xlsx\", 24529)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Extract Entities using GeoBERT\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"botryan96/GeoBERT\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"botryan96/GeoBERT\", from_tf=True)\n",
        "ner_machine = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
        "\n",
        "def extract_entities(text):\n",
        "    # Split the text into chunks of 1000 characters\n",
        "    text_chunks = [text[i:i+1000] for i in range(0, len(text), 1000)]\n",
        "    entities = []\n",
        "    for chunk in text_chunks:\n",
        "        entities += ner_machine(chunk)\n",
        "    return [(entity['word'], entity['entity_group']) for entity in entities]\n",
        "\n",
        "entities = extract_entities(pdf_text)\n",
        "print(entities[:50])  # Print the first 50 entities to check\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C_6UfNZ3Kqr",
        "outputId": "7c6fd91e-354d-41b6-e7b8-c3be34d975a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All TF 2.0 model weights were used when initializing BertForTokenClassification.\n",
            "\n",
            "All the weights of BertForTokenClassification were initialized from the TF 2.0 model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForTokenClassification for predictions without further training.\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('velocity', 'GeoMeth'), ('glacier', 'GeoPetro'), ('##56', 'GeoLoc'), ('remote sensing', 'GeoMeth'), ('pune', 'GeoLoc'), ('remote sensing', 'GeoMeth'), ('velocity', 'GeoMeth'), ('glacier', 'GeoPetro'), ('##dar', 'GeoLoc'), ('velocity', 'GeoMeth'), ('glacier', 'GeoLoc'), ('remote sensing', 'GeoMeth'), ('remote sensing', 'GeoMeth'), ('vol', 'GeoMeth'), ('velocity', 'GeoMeth'), ('glacier', 'GeoPetro'), ('##dar', 'GeoLoc'), ('indian', 'GeoPetro'), ('india', 'GeoLoc'), ('geology', 'GeoMeth'), ('indian', 'GeoPetro'), ('india', 'GeoLoc'), ('earth', 'GeoPetro'), ('ir', 'GeoPetro'), ('##vin', 'GeoPetro'), ('usa', 'GeoLoc'), ('hydrologic', 'GeoMeth'), ('modeling', 'GeoMeth'), ('usa', 'GeoLoc'), ('usa', 'GeoLoc'), ('glacier', 'GeoPetro'), ('water cycle', 'GeoMeth'), ('himalayan', 'GeoPetro'), ('glaciers', 'GeoPetro'), ('india', 'GeoLoc'), ('glacier', 'GeoPetro'), ('aperture', 'GeoMeth'), ('radar', 'GeoMeth'), ('offset', 'GeoMeth'), ('thermal', 'GeoMeth'), ('glacier', 'GeoLoc'), ('glacier', 'GeoLoc'), ('velocity', 'GeoMeth'), ('glacier', 'GeoLoc'), ('glacier', 'GeoLoc'), ('velocity', 'GeoMeth'), ('glacier', 'GeoLoc'), ('uncertainty', 'GeoMeth'), ('glacier', 'GeoLoc'), ('climate change', 'GeoMeth')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Convert the entities list to a pandas DataFrame\n",
        "df = pd.DataFrame(entities, columns=['Entity', 'Label'])\n",
        "\n",
        "# Write the DataFrame to an Excel file\n",
        "df.to_excel('entities_GeoBERT.xlsx', index=False)\n",
        "\n",
        "# Download the file to your local machine\n",
        "files.download('entities_GeoBERT.xlsx')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "geYK1_Yq54f9",
        "outputId": "f42f1755-5a09-44a3-e5a5-0ed1164e0bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_90f031c0-cf4f-47ee-b649-bec2d8b5b248\", \"entities_GeoBERT.xlsx\", 11360)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the ner_machine pipeline on a small text chunk\n",
        "test_chunk = pdf_text[:1000]\n",
        "test_entities = ner_machine(test_chunk)\n",
        "print(test_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQBjqcsn4hcX",
        "outputId": "2d147315-849b-4206-ce6d-4c348e494a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'entity_group': 'GeoMeth', 'score': 0.98896956, 'word': 'velocity', 'start': 102, 'end': 110}, {'entity_group': 'GeoPetro', 'score': 0.70181453, 'word': 'glacier', 'start': 123, 'end': 130}, {'entity_group': 'GeoLoc', 'score': 0.49017197, 'word': '##56', 'start': 197, 'end': 199}, {'entity_group': 'GeoMeth', 'score': 0.96155363, 'word': 'remote sensing', 'start': 237, 'end': 251}, {'entity_group': 'GeoLoc', 'score': 0.90600884, 'word': 'pune', 'start': 293, 'end': 297}, {'entity_group': 'GeoMeth', 'score': 0.9892937, 'word': 'remote sensing', 'start': 732, 'end': 746}, {'entity_group': 'GeoMeth', 'score': 0.9860805, 'word': 'velocity', 'start': 895, 'end': 903}, {'entity_group': 'GeoPetro', 'score': 0.7499879, 'word': 'glacier', 'start': 916, 'end': 923}, {'entity_group': 'GeoLoc', 'score': 0.6558047, 'word': '##dar', 'start': 964, 'end': 967}]\n"
          ]
        }
      ]
    }
  ]
}